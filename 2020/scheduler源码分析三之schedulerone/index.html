<!DOCTYPE html>
<html lang="zh-cn">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="author" content="Yuxiaobo">
  
  
  
  <link rel="prev" href="https://yuxiaobo96.github.io/2020/informer%E6%9C%BA%E5%88%B6/" />
  
  <link rel="canonical" href="https://yuxiaobo96.github.io/2020/scheduler%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%89%E4%B9%8Bschedulerone/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           Scheduler源码分析三之schedulerOne | 将夜泊行
       
  </title>
  <meta name="title" content="Scheduler源码分析三之schedulerOne | 将夜泊行">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/yuxiaobo96.github.io\/"
    },
    "articleSection" : "posts",
    "name" : "Scheduler源码分析三之schedulerOne",
    "headline" : "Scheduler源码分析三之schedulerOne",
    "description" : "在本节主要讲述schedulerOne的基本调度流程，其中用到的预选调度策略、优选调度策略、抢占策略将在后几节介绍。 scheduler源码存",
    "inLanguage" : "zh-cn",
    "author" : "Yuxiaobo",
    "creator" : "Yuxiaobo",
    "publisher": "Yuxiaobo",
    "accountablePerson" : "Yuxiaobo",
    "copyrightHolder" : "Yuxiaobo",
    "copyrightYear" : "2020",
    "datePublished": "2020-01-08 18:38:26 \x2b0800 CST",
    "dateModified" : "2020-01-08 18:38:26 \x2b0800 CST",
    "url" : "https:\/\/yuxiaobo96.github.io\/2020\/scheduler%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%89%E4%B9%8Bschedulerone\/",
    "wordCount" : "5571",
    "keywords" : [  "将夜泊行"]
}
</script>

</head>

  


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
        
        <div class="top-scroll-bar"></div>
    
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://yuxiaobo96.github.io/">将夜泊行</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="">About</a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
        
        <div class="top-scroll-bar"></div>
    
     <div class="container">
        <div class="navbar-header">
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://yuxiaobo96.github.io/">将夜泊行</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="">About</a>
                
        </div>
    </div>
</nav>
    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">Scheduler源码分析三之schedulerOne</h1>
        <div class="post-meta">
                Written by <a itemprop="name" href="https://yuxiaobo96.github.io/" rel="author">Yuxiaobo</a> with ♥ 
                <span class="post-time">
                on <time datetime=2020-01-08 itemprop="datePublished">January 8, 2020</time>
                </span>
                in
                
                <span class="post-word-count">, 5571 words</span>
        </div>
    </header>
    <div class="post-content">
        

        

        
        
     
          
          
          

          
          
          

          

<p>在本节主要讲述schedulerOne的基本调度流程，其中用到的预选调度策略、优选调度策略、抢占策略将在后几节介绍。</p>

<p>scheduler源码存放的另一个文件夹：</p>

<p><code>/pkg/scheduler/</code>，包含scheduler的核心代码，执行scheduler的调度逻辑。文件机构如下：</p>

<pre><code class="language-shell">scheduler
├── algorithm     # 包含scheduler的调度算法
│   ├── BUILD
│   ├── doc.go
│   ├── predicates     # 预选策略
│   ├── priorities     # 优选策略
│   ├── scheduler_interface.go     # 定义SchedulerExtender接口
│   └── types.go
├── algorithm_factory.go
├── algorithm_factory_test.go
├── algorithmprovider
│   ├── BUILD
│   ├── defaults     # 初始化默认算法，包括预选和优选策略
│   ├── plugins.go     # 通过feature gates 应用算法
│   ├── plugins_test.go
│   ├── registry.go     # 注册表：是所有可用的algorithm providers的集合
│   └── registry_test.go
├── api
│   ├── BUILD
│   └── well_known_labels.go     # 外部云供应商在node上设置taint
├── apis
│   ├── config     # kubescheduler调度逻辑所需要的配置
│   └── extender
├── BUILD
├── core     # scheduler的核心代码
│   ├── BUILD
│   ├── extender.go     # 用户配置extender的一系列函数
│   ├── extender_test.go
│   ├── generic_scheduler.go     # 包含调度器的调度逻辑，后面会详细介绍
│   └── generic_scheduler_test.go
├── eventhandlers.go
├── eventhandlers_test.go
├── factory.go
├── factory_test.go
├── framework     # framework框架
│   ├── BUILD
│   ├── plugins     # 包含一系列的plugins，用于校验和过滤
│   └── v1alpha1     # framework的架构设计，包括配置参数及调用plugins
├── internal     # scheduler调度时，使用到的内部资源或工具
│   ├── cache     # scheduler 缓存
│   ├── heap     # heap 主要作用于items
│   └── queue     # 队列，在pod调度前，在队列中排序，通过pop推出pod进行调度
├── listers     # 获取pod和node的信息及表单list
│   ├── BUILD
│   ├── fake
│   ├── listers.go
│   └── listers_test.go
├── metrics     # 一系列指标，与prometheus交互使用
│   ├── BUILD
│   ├── metric_recorder.go
│   ├── metric_recorder_test.go
│   └── metrics.go
├── nodeinfo     # 节点信息，包括节点上已有的pod、volume、taint、resource等
│   ├── BUILD
│   ├── host_ports.go
│   ├── host_ports_test.go
│   ├── node_info.go
│   ├── node_info_test.go
│   └── snapshot    # 为节点创建快照
├── OWNERS
├── scheduler.go     # sched.Run的入口函数及schedulerOne的调度逻辑
├── scheduler_test.go
├── testing     # 测试
│   ├── BUILD
│   ├── framework_helpers.go
│   ├── workload_prep.go
│   └── wrappers.go
├── util
│   ├── BUILD
│   ├── clock.go
│   ├── error_channel.go
│   ├── error_channel_test.go
│   ├── utils.go
│   └── utils_test.go
└── volumebinder     # 绑定 volume
    ├── BUILD
    └── volume_binder.go

</code></pre>

<h2 id="1-sched-run">1. sched.Run</h2>

<blockquote>
<p>代码在<code>pkg/scheduler/scheduler.go</code></p>
</blockquote>

<p>等待缓存同步后，开始scheduler调度逻辑，此处为具体调度逻辑的入口，代码如下：</p>

<pre><code class="language-go">// Run begins watching and scheduling. It waits for cache to be synced, then starts scheduling and blocked until the context is done.
func (sched *Scheduler) Run(ctx context.Context) {
	if !cache.WaitForCacheSync(ctx.Done(), sched.scheduledPodsHasSynced) {
		return
	}

	wait.UntilWithContext(ctx, sched.scheduleOne, 0)
}
</code></pre>

<p>第3行，调用<code>cache.WaitForCacheSync</code>函数，遍历<code>cacheSyncs</code>并使用bool值来判断同步是否完成，代码在<code>staging/src/k8s.io/client-go/tools/cache/shared_informer.go</code>，如下：</p>

<pre><code class="language-go">// WaitForCacheSync waits for caches to populate.  It returns true if it was successful, false
// if the controller should shutdown
// callers should prefer WaitForNamedCacheSync()
func WaitForCacheSync(stopCh &lt;-chan struct{}, cacheSyncs ...InformerSynced) bool {
	err := wait.PollImmediateUntil(syncedPollPeriod,
		func() (bool, error) {
			for _, syncFunc := range cacheSyncs {
				if !syncFunc() {
					return false, nil
				}
			}
			return true, nil
		},
		stopCh)
	if err != nil {
		klog.V(2).Infof(&quot;stop requested&quot;)
		return false
	}

	klog.V(4).Infof(&quot;caches populated&quot;)
	return true
}
</code></pre>

<p>在同一文件下，<code>controller.WaitForNamedCacheSync</code>是对<code>cache.WaitForCacheSync</code>的一层封装，当<code>cache.WaitForCacheSync</code>函数同步失败时，找出不能同步缓存的cotroller名称，记录不同cotroller的缓存同步的日志。</p>

<p><code>controller.WaitForNamedCacheSync</code>的代码如下：</p>

<pre><code class="language-go">// WaitForNamedCacheSync is a wrapper around WaitForCacheSync that generates log messages
// indicating that the caller identified by name is waiting for syncs, followed by
// either a successful or failed sync.
func WaitForNamedCacheSync(controllerName string, stopCh &lt;-chan struct{}, cacheSyncs ...InformerSynced) bool {
	klog.Infof(&quot;Waiting for caches to sync for %s&quot;, controllerName)

	if !WaitForCacheSync(stopCh, cacheSyncs...) {
		utilruntime.HandleError(fmt.Errorf(&quot;unable to sync caches for %s&quot;, controllerName))
		return false
	}

	klog.Infof(&quot;Caches are synced for %s &quot;, controllerName)
	return true
}
</code></pre>

<p>第7行<code>sched.scheduleOne</code>，scheduler的调度逻辑，在标题2详细介绍。</p>

<p><del>第六行<code>sched.SchedulingQueue.Run()</code>，使用队列存储pod，并开启goroutines管理队列</del></p>

<p><del>第八行<code>sched.SchedulingQueue.Close()</code>，关闭SchedulerQueue，等待队列 pop items，goroutine正常退出。</del></p>

<p><del>这里与queue相关的函数和接口在<code>/pkg/scheduler/internal/queue/scheduling_queue.go</code>。</del></p>

<h2 id="2-sched-scheduleone">2. sched.scheduleOne</h2>

<blockquote>
<p>代码在<code>/pkg/scheduler/scheduler.go</code></p>
</blockquote>

<p><code>scheduleOne()</code>函数是对pod调度的完整逻辑：</p>

<ol>
<li>先从队列中使用pop方式弹出需要调度的pod</li>
<li>通过具体的调度策略（预选和优选策略）为该pod筛选出合适的节点</li>
<li>如果上述调度策略失败，则执行抢占式调度逻辑，经过这一系列的操作后选出最佳节点。</li>
<li>将pod设置为assumed状态（与选中的节点进行假性绑定），并执行reserve操作（在选中的节点上为该pod预留资源）</li>
<li>在假性绑定后，继续调度其他的pod。这里注意：pod的调度是同步的，即同一时间只能调度一个pod;而在pod的绑定阶段是异步的,即同一时间可以同时绑定多个pod</li>
<li>调用<code>sched.bind()</code>函数，执行真实绑定，将pod调度到选中的节点上，交给kubelet进行初始化和管理。</li>
<li>绑定结果返回后，将通知cache该pod的绑定已经结束并将结果反馈给apiserver；如果绑定失败，从缓存中删除该pod，并触发unreserve机制，释放节点中为该pod预留的资源。</li>
</ol>

<p>在这个调度逻辑中，还穿插着不同的plugins对pod与node进行过滤，包括QueueSortPlugin、scorePlugins、reservePlugins等。</p>

<p>schedulerOne的具体代码如下：</p>

<pre><code class="language-go">// scheduleOne does the entire scheduling workflow for a single pod.  It is serialized on the scheduling algorithm's host fitting.
func (sched *Scheduler) scheduleOne(ctx context.Context) {
	fwk := sched.Framework

	podInfo := sched.NextPod()
	// pod could be nil when schedulerQueue is closed
	if podInfo == nil || podInfo.Pod == nil {
		return
	}
	pod := podInfo.Pod
	if pod.DeletionTimestamp != nil {
		sched.Recorder.Eventf(pod, nil, v1.EventTypeWarning, &quot;FailedScheduling&quot;, &quot;Scheduling&quot;, &quot;skip schedule deleting pod: %v/%v&quot;, pod.Namespace, pod.Name)
		klog.V(3).Infof(&quot;Skip schedule deleting pod: %v/%v&quot;, pod.Namespace, pod.Name)
		return
	}

	klog.V(3).Infof(&quot;Attempting to schedule pod: %v/%v&quot;, pod.Namespace, pod.Name)

	// Synchronously attempt to find a fit for the pod.
	start := time.Now()
	state := framework.NewCycleState()
	state.SetRecordFrameworkMetrics(rand.Intn(100) &lt; frameworkMetricsSamplePercent)
	schedulingCycleCtx, cancel := context.WithCancel(ctx)
	defer cancel()
	scheduleResult, err := sched.Algorithm.Schedule(schedulingCycleCtx, state, pod)
	if err != nil {
		sched.recordSchedulingFailure(podInfo.DeepCopy(), err, v1.PodReasonUnschedulable, err.Error())
		// Schedule() may have failed because the pod would not fit on any host, so we try to
		// preempt, with the expectation that the next time the pod is tried for scheduling it
		// will fit due to the preemption. It is also possible that a different pod will schedule
		// into the resources that were preempted, but this is harmless.
		if fitError, ok := err.(*core.FitError); ok {
			if sched.DisablePreemption {
				klog.V(3).Infof(&quot;Pod priority feature is not enabled or preemption is disabled by scheduler configuration.&quot; +
					&quot; No preemption is performed.&quot;)
			} else {
				preemptionStartTime := time.Now()
				sched.preempt(schedulingCycleCtx, state, fwk, pod, fitError)
				metrics.PreemptionAttempts.Inc()
				metrics.SchedulingAlgorithmPreemptionEvaluationDuration.Observe(metrics.SinceInSeconds(preemptionStartTime))
				metrics.DeprecatedSchedulingAlgorithmPreemptionEvaluationDuration.Observe(metrics.SinceInMicroseconds(preemptionStartTime))
				metrics.SchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime))
				metrics.DeprecatedSchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime))
			}
			// Pod did not fit anywhere, so it is counted as a failure. If preemption
			// succeeds, the pod should get counted as a success the next time we try to
			// schedule it. (hopefully)
			metrics.PodScheduleFailures.Inc()
		} else {
			klog.Errorf(&quot;error selecting node for pod: %v&quot;, err)
			metrics.PodScheduleErrors.Inc()
		}
		return
	}
	metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInSeconds(start))
	metrics.DeprecatedSchedulingAlgorithmLatency.Observe(metrics.SinceInMicroseconds(start))
	// Tell the cache to assume that a pod now is running on a given node, even though it hasn't been bound yet.
	// This allows us to keep scheduling without waiting on binding to occur.
	assumedPodInfo := podInfo.DeepCopy()
	assumedPod := assumedPodInfo.Pod

	// Assume volumes first before assuming the pod.
	//
	// If all volumes are completely bound, then allBound is true and binding will be skipped.
	//
	// Otherwise, binding of volumes is started after the pod is assumed, but before pod binding.
	//
	// This function modifies 'assumedPod' if volume binding is required.
	allBound, err := sched.VolumeBinder.Binder.AssumePodVolumes(assumedPod, scheduleResult.SuggestedHost)
	if err != nil {
		sched.recordSchedulingFailure(assumedPodInfo, err, SchedulerError,
			fmt.Sprintf(&quot;AssumePodVolumes failed: %v&quot;, err))
		metrics.PodScheduleErrors.Inc()
		return
	}

	// Run &quot;reserve&quot; plugins.
	if sts := fwk.RunReservePlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() {
		sched.recordSchedulingFailure(assumedPodInfo, sts.AsError(), SchedulerError, sts.Message())
		metrics.PodScheduleErrors.Inc()
		return
	}

	// assume modifies `assumedPod` by setting NodeName=scheduleResult.SuggestedHost
	err = sched.assume(assumedPod, scheduleResult.SuggestedHost)
	if err != nil {
		// This is most probably result of a BUG in retrying logic.
		// We report an error here so that pod scheduling can be retried.
		// This relies on the fact that Error will check if the pod has been bound
		// to a node and if so will not add it back to the unscheduled pods queue
		// (otherwise this would cause an infinite loop).
		sched.recordSchedulingFailure(assumedPodInfo, err, SchedulerError, fmt.Sprintf(&quot;AssumePod failed: %v&quot;, err))
		metrics.PodScheduleErrors.Inc()
		// trigger un-reserve plugins to clean up state associated with the reserved Pod
		fwk.RunUnreservePlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
		return
	}
	// bind the pod to its host asynchronously (we can do this b/c of the assumption step above).
	go func() {
		bindingCycleCtx, cancel := context.WithCancel(ctx)
		defer cancel()
		metrics.SchedulerGoroutines.WithLabelValues(&quot;binding&quot;).Inc()
		defer metrics.SchedulerGoroutines.WithLabelValues(&quot;binding&quot;).Dec()

		// Run &quot;permit&quot; plugins.
		permitStatus := fwk.RunPermitPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
		if !permitStatus.IsSuccess() {
			var reason string
			if permitStatus.IsUnschedulable() {
				metrics.PodScheduleFailures.Inc()
				reason = v1.PodReasonUnschedulable
			} else {
				metrics.PodScheduleErrors.Inc()
				reason = SchedulerError
			}
			if forgetErr := sched.Cache().ForgetPod(assumedPod); forgetErr != nil {
				klog.Errorf(&quot;scheduler cache ForgetPod failed: %v&quot;, forgetErr)
			}
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunUnreservePlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
			sched.recordSchedulingFailure(assumedPodInfo, permitStatus.AsError(), reason, permitStatus.Message())
			return
		}

		// Bind volumes first before Pod
		if !allBound {
			err := sched.bindVolumes(assumedPod)
			if err != nil {
				sched.recordSchedulingFailure(assumedPodInfo, err, &quot;VolumeBindingFailed&quot;, err.Error())
				metrics.PodScheduleErrors.Inc()
				// trigger un-reserve plugins to clean up state associated with the reserved Pod
				fwk.RunUnreservePlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
				return
			}
		}

		// Run &quot;prebind&quot; plugins.
		preBindStatus := fwk.RunPreBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
		if !preBindStatus.IsSuccess() {
			var reason string
			metrics.PodScheduleErrors.Inc()
			reason = SchedulerError
			if forgetErr := sched.Cache().ForgetPod(assumedPod); forgetErr != nil {
				klog.Errorf(&quot;scheduler cache ForgetPod failed: %v&quot;, forgetErr)
			}
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunUnreservePlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
			sched.recordSchedulingFailure(assumedPodInfo, preBindStatus.AsError(), reason, preBindStatus.Message())
			return
		}

		err := sched.bind(bindingCycleCtx, assumedPod, scheduleResult.SuggestedHost, state)
		metrics.E2eSchedulingLatency.Observe(metrics.SinceInSeconds(start))
		metrics.DeprecatedE2eSchedulingLatency.Observe(metrics.SinceInMicroseconds(start))
		if err != nil {
			metrics.PodScheduleErrors.Inc()
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunUnreservePlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
			sched.recordSchedulingFailure(assumedPodInfo, err, SchedulerError, fmt.Sprintf(&quot;Binding rejected: %v&quot;, err))
		} else {
			// Calculating nodeResourceString can be heavy. Avoid it if klog verbosity is below 2.
			if klog.V(2) {
				klog.Infof(&quot;pod %v/%v is bound successfully on node %q, %d nodes evaluated, %d nodes were found feasible.&quot;, assumedPod.Namespace, assumedPod.Name, scheduleResult.SuggestedHost, scheduleResult.EvaluatedNodes, scheduleResult.FeasibleNodes)
			}

			metrics.PodScheduleSuccesses.Inc()
			metrics.PodSchedulingAttempts.Observe(float64(podInfo.Attempts))
			metrics.PodSchedulingDuration.Observe(metrics.SinceInSeconds(podInfo.InitialAttemptTimestamp))

			// Run &quot;postbind&quot; plugins.
			fwk.RunPostBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
		}
	}()
}
</code></pre>

<p>在下面对<code>scheduleOne()</code>函数中的重要代码进行分析</p>

<h2 id="3-sched-framework">3. sched.Framework</h2>

<p>第3行，<code>fwk := sched.Framework</code>是声明一些与scheduler相关的plugins作为扩展点</p>

<h2 id="4-nextpod">4. NextPod</h2>

<p>第5行，<code>sched.NextPod()</code>，pod通过queue的方式进行存储，NextPod用来取出下一个待调度的pod。pod的调度队列也是一个核心知识点，将在后续介绍。</p>

<pre><code class="language-go">podInfo := sched.NextPod()
	// pod could be nil when schedulerQueue is closed
	if podInfo == nil || podInfo.Pod == nil {
		return
	}
	pod := podInfo.Pod
	if pod.DeletionTimestamp != nil {
		sched.Recorder.Eventf(pod, nil, v1.EventTypeWarning, &quot;FailedScheduling&quot;, &quot;Scheduling&quot;, &quot;skip schedule deleting pod: %v/%v&quot;, pod.Namespace, pod.Name)
		klog.V(3).Infof(&quot;Skip schedule deleting pod: %v/%v&quot;, pod.Namespace, pod.Name)
		return
	}
</code></pre>

<p>其中：在第7行，<code>pod.DeletionTimestamp</code>，检查要调度的pod的时间戳，如果pod的<code>DeletionTimestamp</code>不为空，则直接返回，因为这个pod已经被下达了删除命令，不需要进行调度。</p>

<h3 id="5-设定预选的节点数量">5. 设定预选的节点数量</h3>

<p>在第19行，代码如下：</p>

<pre><code class="language-go">// Synchronously attempt to find a fit for the pod.
	start := time.Now()
	state := framework.NewCycleState()
	state.SetRecordFrameworkMetrics(rand.Intn(100) &lt; frameworkMetricsSamplePercent)
	schedulingCycleCtx, cancel := context.WithCancel(ctx)
	defer cancel()
</code></pre>

<p>在开始调度pod之前，先设置筛选出节点的容量，这种做法是为了避免：当集群内的节点过多时，将所有节点过滤一遍，浪费资源和时间。可以看到代码里使用的percent（百分比）的概念，进入到<code>frameworkMetricsSamplePercent</code>常量后，可以发现：</p>

<pre><code class="language-go">// Percentage of framework metrics to be sampled.
	frameworkMetricsSamplePercent = 10
</code></pre>

<p>这里将节点的筛选百分比设置成百分之十，有以下几种情况：</p>

<ol>
<li>当节点数过多并超过100时，只筛选这些节点总数的百分之十作为最后筛选出的节点。</li>
<li>当节点数小于100时，则将所有节点筛选一遍</li>
</ol>

<h2 id="6-sched-algorithm-schedule">6. sched.Algorithm.Schedule</h2>

<p>第25行，此处是scheduelr两个调度策略（预选和优选策略）的核心逻辑。</p>

<blockquote>
<p>代码在<code>/pkg/scheduler/core/generic_scheduler.go</code></p>
</blockquote>

<p>具体代码如下：</p>

<pre><code class="language-go">// Schedule tries to schedule the given pod to one of the nodes in the node list.
// If it succeeds, it will return the name of the node.
// If it fails, it will return a FitError error with reasons.
func (g *genericScheduler) Schedule(ctx context.Context, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
	trace := utiltrace.New(&quot;Scheduling&quot;, utiltrace.Field{Key: &quot;namespace&quot;, Value: pod.Namespace}, utiltrace.Field{Key: &quot;name&quot;, Value: pod.Name})
	defer trace.LogIfLong(100 * time.Millisecond)

	if err := podPassesBasicChecks(pod, g.pvcLister); err != nil {
		return result, err
	}
	trace.Step(&quot;Basic checks done&quot;)

	if err := g.snapshot(); err != nil {
		return result, err
	}
	trace.Step(&quot;Snapshoting scheduler cache and node infos done&quot;)

	if len(g.nodeInfoSnapshot.NodeInfoList) == 0 {
		return result, ErrNoNodesAvailable
	}

	// Run &quot;prefilter&quot; plugins.
	preFilterStatus := g.framework.RunPreFilterPlugins(ctx, state, pod)
	if !preFilterStatus.IsSuccess() {
		return result, preFilterStatus.AsError()
	}
	trace.Step(&quot;Running prefilter plugins done&quot;)

	startPredicateEvalTime := time.Now()
	filteredNodes, failedPredicateMap, filteredNodesStatuses, err := g.findNodesThatFit(ctx, state, pod)
	if err != nil {
		return result, err
	}
	trace.Step(&quot;Computing predicates done&quot;)

	// Run &quot;postfilter&quot; plugins.
	postfilterStatus := g.framework.RunPostFilterPlugins(ctx, state, pod, filteredNodes, filteredNodesStatuses)
	if !postfilterStatus.IsSuccess() {
		return result, postfilterStatus.AsError()
	}

	if len(filteredNodes) == 0 {
		return result, &amp;FitError{
			Pod:                   pod,
			NumAllNodes:           len(g.nodeInfoSnapshot.NodeInfoList),
			FailedPredicates:      failedPredicateMap,
			FilteredNodesStatuses: filteredNodesStatuses,
		}
	}
	trace.Step(&quot;Running postfilter plugins done&quot;)
	metrics.SchedulingAlgorithmPredicateEvaluationDuration.Observe(metrics.SinceInSeconds(startPredicateEvalTime))
	metrics.DeprecatedSchedulingAlgorithmPredicateEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPredicateEvalTime))
	metrics.SchedulingLatency.WithLabelValues(metrics.PredicateEvaluation).Observe(metrics.SinceInSeconds(startPredicateEvalTime))
	metrics.DeprecatedSchedulingLatency.WithLabelValues(metrics.PredicateEvaluation).Observe(metrics.SinceInSeconds(startPredicateEvalTime))

	startPriorityEvalTime := time.Now()
	// When only one node after predicate, just use it.
	if len(filteredNodes) == 1 {
		metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInSeconds(startPriorityEvalTime))
		metrics.DeprecatedSchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime))
		return ScheduleResult{
			SuggestedHost:  filteredNodes[0].Name,
			EvaluatedNodes: 1 + len(failedPredicateMap) + len(filteredNodesStatuses),
			FeasibleNodes:  1,
		}, nil
	}

	metaPrioritiesInterface := g.priorityMetaProducer(pod, filteredNodes, g.nodeInfoSnapshot)
	priorityList, err := g.prioritizeNodes(ctx, state, pod, metaPrioritiesInterface, filteredNodes)
	if err != nil {
		return result, err
	}

	metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInSeconds(startPriorityEvalTime))
	metrics.DeprecatedSchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime))
	metrics.SchedulingLatency.WithLabelValues(metrics.PriorityEvaluation).Observe(metrics.SinceInSeconds(startPriorityEvalTime))
	metrics.DeprecatedSchedulingLatency.WithLabelValues(metrics.PriorityEvaluation).Observe(metrics.SinceInSeconds(startPriorityEvalTime))

	host, err := g.selectHost(priorityList)
	trace.Step(&quot;Prioritizing done&quot;)

	return ScheduleResult{
		SuggestedHost:  host,
		EvaluatedNodes: len(filteredNodes) + len(failedPredicateMap) + len(filteredNodesStatuses),
		FeasibleNodes:  len(filteredNodes),
	}, err
}
</code></pre>

<p>接下来对<code>Scheduler()</code>函数中的重要代码进行分析</p>

<h3 id="6-1-podpassesbasicchecks">6.1 podPassesBasicChecks</h3>

<p>第8行，podPassesBasicChecks对pod进行一些基本检查，目前只对pvc（PersistentVolumeClaim）进行检查。</p>

<pre><code class="language-go">if err := podPassesBasicChecks(pod, g.pvcLister); err != nil {
		return result, err
	}
	trace.Step(&quot;Basic checks done&quot;)
</code></pre>

<p><code>podPassesBasicChecks()</code>函数的具体实现如下：</p>

<pre><code class="language-go">// podPassesBasicChecks makes sanity checks on the pod if it can be scheduled.
func podPassesBasicChecks(pod *v1.Pod, pvcLister corelisters.PersistentVolumeClaimLister) error {
	// Check PVCs used by the pod
	namespace := pod.Namespace
	manifest := &amp;(pod.Spec)
	for i := range manifest.Volumes {
		volume := &amp;manifest.Volumes[i]
		if volume.PersistentVolumeClaim == nil {
			// Volume is not a PVC, ignore
			continue
		}
		pvcName := volume.PersistentVolumeClaim.ClaimName
		pvc, err := pvcLister.PersistentVolumeClaims(namespace).Get(pvcName)
		if err != nil {
			// The error has already enough context (&quot;persistentvolumeclaim &quot;myclaim&quot; not found&quot;)
			return err
		}

		if pvc.DeletionTimestamp != nil {
			return fmt.Errorf(&quot;persistentvolumeclaim %q is being deleted&quot;, pvc.Name)
		}
	}

	return nil
}

</code></pre>

<h3 id="6-2-snapshot">6.2 snapshot</h3>

<p>第13行，<code>snapshot()</code>将遍历cache中的所有node，更新node的信息，包括：cache NodeInfo and NodeTree order。此snapshot作用于所有预选和优选函数</p>

<pre><code class="language-go">if err := g.snapshot(); err != nil {
		return result, err
	}
	trace.Step(&quot;Snapshoting scheduler cache and node infos done&quot;)
</code></pre>

<p><code>snapshot()</code>函数的具体实现如下：</p>

<pre><code class="language-go">// snapshot snapshots scheduler cache and node infos for all fit and priority
// functions.
func (g *genericScheduler) snapshot() error {
	// Used for all fit and priority funcs.
	return g.cache.UpdateNodeInfoSnapshot(g.nodeInfoSnapshot)
}
</code></pre>

<h3 id="6-4-nodeinfolist">6.4 NodeInfoList</h3>

<p>第18行，对node的列表长度进行判断，检查是否有可用的节点。</p>

<pre><code class="language-go">if len(g.nodeInfoSnapshot.NodeInfoList) == 0 {
		return result, ErrNoNodesAvailable
	}
</code></pre>

<h3 id="6-5-findnodesthatfit">6.5 findNodesThatFit</h3>

<blockquote>
<p>具体<code>findNodesThatFit</code>的代码细节将在后续的独立文章中分析</p>
</blockquote>

<p>第30行，<code>g.findNodesThatFit()</code>函数是预选策略的调度逻辑。</p>

<pre><code class="language-go">startPredicateEvalTime := time.Now()
	filteredNodes, failedPredicateMap, filteredNodesStatuses, err := g.findNodesThatFit(ctx, state, pod)
	if err != nil {
		return result, err
	}
	trace.Step(&quot;Computing predicates done&quot;)
</code></pre>

<h3 id="6-6-prioritizenodes">6.6 prioritizeNodes</h3>

<blockquote>
<p>具体<code>prioritizeNodes</code>的代码细节将在后续的独立文章中分析</p>
</blockquote>

<p>第56行，<code>g.priorityMetaProducer()</code>函数是优选策略的调度逻辑，优选策略是对预选出的节点进行二次筛选，</p>

<pre><code class="language-go">startPriorityEvalTime := time.Now()
	// When only one node after predicate, just use it.
	if len(filteredNodes) == 1 {
		metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInSeconds(startPriorityEvalTime))
		metrics.DeprecatedSchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime))
		return ScheduleResult{
			SuggestedHost:  filteredNodes[0].Name,
			EvaluatedNodes: 1 + len(failedPredicateMap) + len(filteredNodesStatuses),
			FeasibleNodes:  1,
		}, nil
	}

	metaPrioritiesInterface := g.priorityMetaProducer(pod, filteredNodes, g.nodeInfoSnapshot)
	priorityList, err := g.prioritizeNodes(ctx, state, pod, metaPrioritiesInterface, filteredNodes)
	if err != nil {
		return result, err
	}
</code></pre>

<p>其中：</p>

<p>第3行，对预选出的节点进行判断：当节点的个数为1时，直接使用这个节点，就不需要再进行优选策略了，这是检验的正常逻辑。</p>

<p>第14行，这里返回的值是<code>priorityList</code>，表明优选策略选出的是包含多个最佳节点的列表，而不是选出唯一的一个节点，优选策略是通过score的计算来对节点进行过滤的。</p>

<h3 id="6-7-selecthost">6.7 selectHost</h3>

<p>第79行，<code>g.selectHost()</code>函数的参数为<code>priorityList</code>，即优选策略选出的最佳节点的列表。selectHost从这个列表出选出分数最高的节点，作为最后pod绑定的节点。当有多个节点的分数相同时，在k8s中定义了一个更详细的选择算法，这里就不叙述了。</p>

<pre><code class="language-go">host, err := g.selectHost(priorityList)
	trace.Step(&quot;Prioritizing done&quot;)
</code></pre>

<p><code>selectHost()</code>函数的具体实现为：</p>

<pre><code class="language-go">// selectHost takes a prioritized list of nodes and then picks one
// in a reservoir sampling manner from the nodes that had the highest score.
func (g *genericScheduler) selectHost(nodeScoreList framework.NodeScoreList) (string, error) {
	if len(nodeScoreList) == 0 {
		return &quot;&quot;, fmt.Errorf(&quot;empty priorityList&quot;)
	}
	maxScore := nodeScoreList[0].Score
	selected := nodeScoreList[0].Name
	cntOfMaxScore := 1
	for _, ns := range nodeScoreList[1:] {
		if ns.Score &gt; maxScore {
			maxScore = ns.Score
			selected = ns.Name
			cntOfMaxScore = 1
		} else if ns.Score == maxScore {
			cntOfMaxScore++
			if rand.Intn(cntOfMaxScore) == 0 {
				// Replace the candidate with probability of 1/cntOfMaxScore
				selected = ns.Name
			}
		}
	}
	return selected, nil
}
</code></pre>

<p>这个函数的逻辑就是对节点的分数进行比较，最后选出节点分数最高的那个节点，返回结果。</p>

<h2 id="7-sched-preempt">7.sched.preempt</h2>

<blockquote>
<p>具体的<code>sched.preempt</code>的代码细节将在后续的文章单独分析</p>
</blockquote>

<p>第37行，<code>sched.preempt()</code>函数是抢占逻辑的实现。当预选和优选策略均失败时，就会进行抢占式调度，将某个节点上的低优先级的pod驱逐，将优先级更高的pod调度到这个节点上。</p>

<pre><code class="language-go">	if err != nil {
		sched.recordSchedulingFailure(podInfo.DeepCopy(), err, v1.PodReasonUnschedulable, err.Error())
		// Schedule() may have failed because the pod would not fit on any host, so we try to
		// preempt, with the expectation that the next time the pod is tried for scheduling it
		// will fit due to the preemption. It is also possible that a different pod will schedule
		// into the resources that were preempted, but this is harmless.
		if fitError, ok := err.(*core.FitError); ok {
			if sched.DisablePreemption {
				klog.V(3).Infof(&quot;Pod priority feature is not enabled or preemption is disabled by scheduler configuration.&quot; +
					&quot; No preemption is performed.&quot;)
			} else {
				preemptionStartTime := time.Now()
				sched.preempt(schedulingCycleCtx, state, fwk, pod, fitError)
				metrics.PreemptionAttempts.Inc()
				metrics.SchedulingAlgorithmPreemptionEvaluationDuration.Observe(metrics.SinceInSeconds(preemptionStartTime))
				metrics.DeprecatedSchedulingAlgorithmPreemptionEvaluationDuration.Observe(metrics.SinceInMicroseconds(preemptionStartTime))
				metrics.SchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime))
				metrics.DeprecatedSchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime))
			}
			// Pod did not fit anywhere, so it is counted as a failure. If preemption
			// succeeds, the pod should get counted as a success the next time we try to
			// schedule it. (hopefully)
			metrics.PodScheduleFailures.Inc()
		} else {
			klog.Errorf(&quot;error selecting node for pod: %v&quot;, err)
			metrics.PodScheduleErrors.Inc()
		}
		return
</code></pre>

    </div>

    <div class="post-copyright">
             
            <p class="copyright-item">
                <span>Author:</span>
                <span>Yuxiaobo </span>
                </p>
            
           
             
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=https://yuxiaobo96.github.io/2020/scheduler%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%89%E4%B9%8Bschedulerone/>https://yuxiaobo96.github.io/2020/scheduler%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%89%E4%B9%8Bschedulerone/</span>
            </p>
            
            
    </div>

  
    <div class="post-tags">
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> · 
                <span><a href="https://yuxiaobo96.github.io/">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://yuxiaobo96.github.io/2020/informer%E6%9C%BA%E5%88%B6/" class="prev" rel="prev" title="Informer机制"><i class="iconfont icon-left"></i>&nbsp;Informer机制</a>
         
        
    </div>

    <div class="post-comment">
          
                 
          
        
<div id="gitalk-container"></div>
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script>
  const gitalk = new Gitalk({
    clientID: 'cdf2110576d2e664531d',
    clientSecret: '6a6c277a187f8ecef693d89e334a942aee820a96',
    repo: 'yuxiaobo96.github.io',
    owner: 'yuxiaobo96',
    admin: ['yuxiaobo96'],
    id: location.pathname, 
    distractionFreeMode: false 
  });
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('gitalk-container').innerHTML = 'Gitalk comments not available by default when the website is previewed locally.';
      return;
    }
    gitalk.render('gitalk-container');
  })();
</script>

    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2019 - 2020</span>
        
        <span class="with-love">
    	 <i class="iconfont icon-love"></i> 
         </span>
         
            <span class="author" itemprop="copyrightHolder"><a href="https://yuxiaobo96.github.io/">Yuxiaobo</a> | </span> 
         

         
		  <span>Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/liuzc/leaveit" target="_blank" rel="external nofollow">LeaveIt</a></span> 
    </div>
</footer>













    
    
    <script src="/js/vendor_no_gallery.min.js" async=""></script>
    
  



     </div>
  </body>
</html>
